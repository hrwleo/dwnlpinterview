# 【关于 batch_size设置】那些你不知道的事

- [【关于 batch_size设置】那些你不知道的事](#关于-batch_size设置那些你不知道的事)
  - [一、训练模型时，batch_size的设置，学习率的设置?](#一训练模型时batch_size的设置学习率的设置)


## 一、训练模型时，batch_size的设置，学习率的设置?

1. cs224里说过，因为NVIDIA底层的并行计算架构，一般的训练batchsize32和64是效果最好的～好像也有实验支持。如果是训练GAN的话越大越好，biggan那篇paper有做过实验。学习率的设置就是调参的一部分啦，一般训练中会设置学习率衰减

![](img/batch_size.png)

2. 传入的时候是矩阵，宏观上是同时，微观上就是硬件底层运算，biggan的batchsize是2048，我记得作者做过实验再大的话就会训练不稳定并且提升已经很低了。
3. batch一般选取2^n，主要是为了符合计算机内部的计算
4. 大的batchsize减少训练时间，提高稳定性，计算更加稳定，因为模型训练曲线会更加平滑，但是会导致模型泛化能力下降，而小的batchsize会会有更好的泛化能力从而逃离sharp minimum，当我们增加batchsize为原来的N倍时，要保证经过同样的样本后更新的权重相等，按照线性缩放规则，学习率应该增加为原来的N倍，但是如果要保证权重的方差不变，则学习率应该增加为原来的sqrt(N)倍
